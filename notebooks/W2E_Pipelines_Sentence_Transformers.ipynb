{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2RjQLPDARv_"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gsarti/ik-nlp-tutorials/blob/main/notebooks/W2E_Pipelines_Sentence_Transformers.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AiCXPBcOARwA",
        "outputId": "ad637c56-d62d-4ba3-ab27-0e2f71d31fb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Run in Colab to install local packages\n",
        "!pip install transformers sentencepiece torch datasets sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjH6ueP8ARwA"
      },
      "source": [
        "# 🤗 Pipelines & Sentence Transformers for Semantic Search and QA\n",
        "\n",
        "*This notebook is partly adapted from a tutorial by Wietse De Vries for the IK-NLP course of 2021*\n",
        "\n",
        "The goal of this notebook is to make you practice with some pipeline use-cases and to introduce the [Sentence Transformers](https://sbert.net/) in the context of a concrete example of semantic search and question answering (relevant to the Open-book Question Answering final project).\n",
        "\n",
        "Exercises 1 and 2 of this notebook are mandatory and will be part of your first graded portfolio. Exercise 3 is optional, but we highly recommend you to complete it, especially if you're interested in the Open-book Question Answering final project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPd8XKAYARwB"
      },
      "source": [
        "## Exercise 1: Using the Fill-mask pipeline for Probing Linguistic Knowledge in mBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzbajhLKARwB"
      },
      "source": [
        "As you probably know by now, BERT is a transformed-based, context-sensitive, neural language models that has been trained, among others, on a masked language modeling task, where the model learns to predict what the most likely word is at a a *masked* (hidden) position in the sentence. In a sentence like:\n",
        "\n",
        ">There were several [MASK] with the proposed solution.\n",
        "    \n",
        "the model will learn that the word *problems* or *issues* is more likely at this position than the word *unicorns* or *days*. The model uses both the right and left context of the masked position to make its predictions. Models trained to maximize the probability of predicting the correct masked words learn to represent a good amount of linguistic knowledge as a result of this.\n",
        "\n",
        "A **probe** is a test of a language model aimed at investigating how accurate these predictions are, especially for cases where syntax makes it quite clear that one (form of a) word is correct, and another word is impossible. In the example above, for instance, the masked position can be filled by a plural noun (*problems*) but not by a singular noun (*problem*). If the model makes predictions that respect the linguistic constraints, we have reason to believe that the model is somehow aware of the linguistic structure of the language.\n",
        "\n",
        "While predicting whether the masked position should be filled by a singular or plural noun seems easy in the example above (both *were* and *several* are good predictors of plural), we can try to make the task harder by looking for contexts where the solution requires more careful *attention* to the right words in the context\n",
        "\n",
        ">There were some [MASK] with the proposed solution.\n",
        ">\n",
        ">There could be several [MASK] with the proposed solution.\n",
        ">\n",
        ">There were some unexpected and unforeseen [MASK] with the proposed solution.\n",
        "    \n",
        "In the examples above, the task is made harder by replacing *several* (which is always followed by a plural noun) by *some* (which can be followed by a singular or a plural noun), by replacing *were* (which always heads a sentence with a plural subject) by *could be* (which can head a sentence with a singular or plural subject), and by inserting material between the verb *were* (which indicates that there should be a plural) and the MASK.\n",
        "\n",
        "\n",
        "### Assignment\n",
        "\n",
        "Think of a grammatical phenomenon in a language of your choice, and come up with at least 5 example sentences to probe whether the model makes the correct predictions. Think of cases where the context makes it clear that the mask has to be plural or singular, that a verb has to have a particular form (like plural or singular, or participle or infinitive), that a specific (personal, possessive, reflexive) pronoun has to be used, that an adjective or noun has to have a specific inflection (like in German and more generally in languages with a rich case and/or gender marking system). There is a host of literature on this, see for instance [Marvin and Linzen](https://arxiv.org/abs/1808.09031) (for English) and [Sahin et al](https://www.mitpressjournals.org/doi/full/10.1162/coli_a_00376) (for multilingual probes).\n",
        "\n",
        "### Model\n",
        "\n",
        "The model we will be using for this task is the [multilingual BERT](https://huggingface.co/bert-base-multilingual-cased) model mBERT, that was trained on the Wikipedia text of the 102 largest Wikipedia's. This means that you do not have to choose examples from English, but that you may also present a probe for another language.\n",
        "\n",
        "The following loads the pipeline for doing masked prediction, and load the mBERT model (this may take a minute or so). You can ignore the warning about some weights not being initialized. The pipeline can be used to test masked language model prediction. Given a sequence containing the special token [MASK], the model will predict what the most likely tokens are at that position, using both left and right context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBO9DDS1ARwB",
        "outputId": "ef23e744-bf61-4ba0-e0c5-5e71d31fc48b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'sequence': 'There were some unexpected and unforeseen problems with the proposed solution.',\n",
              "  'score': 0.5272422432899475,\n",
              "  'token': 20390,\n",
              "  'token_str': 'problems'},\n",
              " {'sequence': 'There were some unexpected and unforeseen issues with the proposed solution.',\n",
              "  'score': 0.09590281546115875,\n",
              "  'token': 17850,\n",
              "  'token_str': 'issues'},\n",
              " {'sequence': 'There were some unexpected and unforeseen difficulties with the proposed solution.',\n",
              "  'score': 0.0664457157254219,\n",
              "  'token': 64557,\n",
              "  'token_str': 'difficulties'},\n",
              " {'sequence': 'There were some unexpected and unforeseen dealing with the proposed solution.',\n",
              "  'score': 0.0168981421738863,\n",
              "  'token': 73082,\n",
              "  'token_str': 'dealing'},\n",
              " {'sequence': 'There were some unexpected and unforeseen associated with the proposed solution.',\n",
              "  'score': 0.01516666542738676,\n",
              "  'token': 18107,\n",
              "  'token_str': 'associated'}]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "mbert = pipeline('fill-mask', model='bert-base-multilingual-cased')\n",
        "mbert('There were some unexpected and unforeseen [MASK] with the proposed solution.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prUwHl2AARwC"
      },
      "source": [
        "By default, the pipe returns the 5 most likely words that could appear at the position of the mask, along with a score. If you want to know specifically whether the model prefers one of two forms, you can give these forms as targets to the pipe, and also print the answer in a more readable form:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebg-h1WrARwC",
        "outputId": "624d472e-eb54-435a-ac68-b9a248d1dc0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0034\ttoken: challenges\tThere were some unexpected and unforeseen challenges with the proposed solution.\n",
            "0.0001\ttoken: challenge\tThere were some unexpected and unforeseen challenge with the proposed solution.\n"
          ]
        }
      ],
      "source": [
        "def probe(sentence: str, targets: str) :\n",
        "    for res in mbert(sentence,targets=targets) :\n",
        "        print(f\"{res['score']:6.4f}\\ttoken: {res['token_str']}\\t{res['sequence']}\")\n",
        "\n",
        "probe('There were some unexpected and unforeseen [MASK] with the proposed solution.',['challenge','challenges'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hto6ZH8sARwC"
      },
      "source": [
        "> **💡 Interesting Fact**: The same bias that is present towards grammatically correct choices can be observed in other cases, such as racial and gender stereotyping. Much work is currently in process to identify and remove gender and racial biases from learned language embeddings. See the following example and [this recent survey on the topic](https://arxiv.org/abs/2112.14168)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmEtejPXARwC",
        "outputId": "856b2cf4-b525-4205-8f47-900fd9624a7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']\n",
            "['nurse', 'waitress', 'teacher', 'maid', 'prostitute']\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-cased\")\n",
        "result = unmasker(\"This man works as a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "result = unmasker(\"This woman works as a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSy5SuW7ARwD"
      },
      "source": [
        "### Your Turn to Probe\n",
        "\n",
        "Give at least five example sentences with a [MASK] and a list of targets that illustrate a specific grammatical phenomenon in a language of your choice. Describe what the grammatical phenomenon is you are investigating. Use the probe function for testing. Try to include both *easy* sentences (where the model should do well) as well as *hard* sentences (where there are words in the context that might lead to confusion, or where the clue words are far away from the mask). For languages other than Dutch or English, make sure to include enough explanation so that examples and tests are clear to a non-native speaker.\n",
        "\n",
        "Describe how well the model did on your probe sentences. Where there any cases where the model made the wrong decision?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xl2tMU6fARwD",
        "outputId": "57697838-62f2-4339-9d1e-386044ff8c40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results for sentence 1:\n",
            "{'score': 0.5024310350418091, 'token': 10445, 'token_str': 'Der', 'sequence': 'Der Mann läuft im Park.'}\n",
            "{'score': 0.3762442469596863, 'token': 12210, 'token_str': 'Ein', 'sequence': 'Ein Mann läuft im Park.'}\n",
            "{'score': 0.008265103213489056, 'token': 29224, 'token_str': 'Ihr', 'sequence': 'Ihr Mann läuft im Park.'}\n",
            "{'score': 0.007685049902647734, 'token': 105818, 'token_str': 'Jeder', 'sequence': 'Jeder Mann läuft im Park.'}\n",
            "{'score': 0.00671159103512764, 'token': 49107, 'token_str': 'Mein', 'sequence': 'Mein Mann läuft im Park.'}\n",
            "\n",
            "results for sentence 2:\n",
            "{'score': 0.5928909778594971, 'token': 10128, 'token_str': 'die', 'sequence': 'Ich betrachte die Blume im Garten.'}\n",
            "{'score': 0.21719825267791748, 'token': 10359, 'token_str': 'eine', 'sequence': 'Ich betrachte eine Blume im Garten.'}\n",
            "{'score': 0.0740298181772232, 'token': 11493, 'token_str': 'seine', 'sequence': 'Ich betrachte seine Blume im Garten.'}\n",
            "{'score': 0.020195266231894493, 'token': 99864, 'token_str': 'meine', 'sequence': 'Ich betrachte meine Blume im Garten.'}\n",
            "{'score': 0.019027430564165115, 'token': 12750, 'token_str': 'diese', 'sequence': 'Ich betrachte diese Blume im Garten.'}\n",
            "\n",
            "results for sentence 3:\n",
            "{'score': 0.4165107011795044, 'token': 12651, 'token_str': 'seinem', 'sequence': 'Er hilft seinem Nachbar bei den Reparaturen.'}\n",
            "{'score': 0.16970492899417877, 'token': 12724, 'token_str': 'seinen', 'sequence': 'Er hilft seinen Nachbar bei den Reparaturen.'}\n",
            "{'score': 0.12165681272745132, 'token': 10268, 'token_str': 'dem', 'sequence': 'Er hilft dem Nachbar bei den Reparaturen.'}\n",
            "{'score': 0.04059101641178131, 'token': 10223, 'token_str': 'als', 'sequence': 'Er hilft als Nachbar bei den Reparaturen.'}\n",
            "{'score': 0.03253207728266716, 'token': 11479, 'token_str': 'sein', 'sequence': 'Er hilft sein Nachbar bei den Reparaturen.'}\n",
            "\n",
            "results for sentence 4:\n",
            "{'score': 0.7136588096618652, 'token': 10140, 'token_str': 'den', 'sequence': 'Das ist der Hund, der den Besitzer liebevoll pflegt.'}\n",
            "{'score': 0.1001683697104454, 'token': 12724, 'token_str': 'seinen', 'sequence': 'Das ist der Hund, der seinen Besitzer liebevoll pflegt.'}\n",
            "{'score': 0.03835950046777725, 'token': 10118, 'token_str': 'der', 'sequence': 'Das ist der Hund, der der Besitzer liebevoll pflegt.'}\n",
            "{'score': 0.03221149370074272, 'token': 10268, 'token_str': 'dem', 'sequence': 'Das ist der Hund, der dem Besitzer liebevoll pflegt.'}\n",
            "{'score': 0.024692513048648834, 'token': 10128, 'token_str': 'die', 'sequence': 'Das ist der Hund, der die Besitzer liebevoll pflegt.'}\n",
            "\n",
            "results for sentence 5:\n",
            "{'score': 0.9425893425941467, 'token': 10139, 'token_str': 'des', 'sequence': 'Das ist das Rezept des Bäckers, das er seit Jahren perfektioniert.'}\n",
            "{'score': 0.023843176662921906, 'token': 11655, 'token_str': 'eines', 'sequence': 'Das ist das Rezept eines Bäckers, das er seit Jahren perfektioniert.'}\n",
            "{'score': 0.018139779567718506, 'token': 10118, 'token_str': 'der', 'sequence': 'Das ist das Rezept der Bäckers, das er seit Jahren perfektioniert.'}\n",
            "{'score': 0.009075110778212547, 'token': 16970, 'token_str': 'seines', 'sequence': 'Das ist das Rezept seines Bäckers, das er seit Jahren perfektioniert.'}\n",
            "{'score': 0.0006367148598656058, 'token': 11036, 'token_str': 'vom', 'sequence': 'Das ist das Rezept vom Bäckers, das er seit Jahren perfektioniert.'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "mbert = pipeline('fill-mask', model='bert-base-multilingual-cased')\n",
        "\n",
        "sentences = [\n",
        "    \"[MASK] Mann läuft im Park.\",\n",
        "    \"Ich betrachte [MASK] Blume im Garten.\",\n",
        "    \"Er hilft [MASK] Nachbar bei den Reparaturen.\",\n",
        "    \"Das ist der Hund, der [MASK] Besitzer liebevoll pflegt.\",\n",
        "    \"Das ist das Rezept [MASK] Bäckers, das er seit Jahren perfektioniert.\"\n",
        "]\n",
        "\n",
        "results = mbert(sentences)\n",
        "\n",
        "for i, sentence_results in enumerate (results, start=1):\n",
        "  print(f\"results for sentence {i}:\")\n",
        "  for candidate in sentence_results:\n",
        "    print(candidate)\n",
        "  print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNaNg6n6ARwD"
      },
      "source": [
        "## Exercise 2: Mixing Pipelines for Text-to-text QA in Many Languages\n",
        "\n",
        "The Model Hub of HuggingFace is home to a staggering amount of models for the more disparate use-cases, but you may have noticed that many of those are trained on the English language. Let's consider for example the [`UnifiedQA`](https://github.com/allenai/unifiedqa) model by AllenAI, which is a T5 model architecture trained to perform question answering on multiple formats (e.g. extract the answer from the provided context, produce an answer without a supporting context, choose among multiple possible answers, yes/no questions) using a unified text-to-text approach. While this opens thrilling perspectives in having a single model for all QA use-cases, UnifiedQA models are available for English only, and training such models from scratch in another language would require nontrivial effort and resources.\n",
        "\n",
        "Here are several examples of how text should be formatted for the UnifiedQA model:\n",
        "\n",
        "| **Task type** | **Example Dataset** | **Format** | **Example** | **Output** |\n",
        "| :---: | :--- | :--- | :--- | :--- |\n",
        "|  **Extractive QA** | SQUAD | `<QUESTION> \\n <CONTEXT>` | `At what speed did the turbine operate? \\n (Nikola_Tesla) On his 50th birthday in 1906, Tesla demonstrated his 200 horsepower (150 kilowatts) 16,000 rpm bladeless turbine. ...` |  `16,000 rpm` |\n",
        "|  **Abstractive QA** | NarrativeQA | `<QUESTION> \\n <CONTEXT>` | `What does a drink from narcissus's spring cause the drinker to do?  \\n  Mercury has awakened Echo, who weeps for Narcissus, and states that a drink from Narcissus's spring causes the drinkers to ''Grow dotingly enamored of themselves.'' ...` | `fall in love with themselves` |\n",
        "|  **Multiple-choice QA** | ARC-challenge | `<QUESTION> \\n (a) <CHOICE_A> (b) <CHOICE_B> ...` | `What does photosynthesis produce that helps plants grow? \\n (A) water (B) oxygen (C) protein (D) sugar` | `sugar` |\n",
        "|  **Multiple-choice QA with context** | MCTest | `<QUESTION> \\n (a) <CHOICE_A> (b) <CHOICE_B> ... \\n <CONTEXT>` | `Who was Billy? \\n (A) The skinny kid (B) A teacher (C) A little kid (D) The big kid \\n Billy was like a king on the school yard. A king without a queen. He was the biggest kid in our grade, so he made all the rules during recess. ...` | `The big kid` |\n",
        "|  **Yes-no QA** | BoolQ | `<QUESTION> \\n <CONTEXT>` | `Was America the first country to have a president?  \\n (President) The first usage of the word president to denote the highest official in a government was during the Commonwealth of England ...` | `no` |\n",
        "\n",
        "### Assignment\n",
        "\n",
        "We are gonna build a function making use of multiple models through 🤗 Pipelines to generate a response to a question in one of the formats specified above, in one of the languages supported by the MT systems available on the HuggingFace Model Hub. The function will translate and paraphrase the query into multiple examples, and then pick the best outputs of the UnifiedQA model as candidates for backtranslation. In this way, we mock the existance of a UnifiedQA model for the language of our choice.\n",
        "\n",
        "### Model\n",
        "\n",
        "The following code loads the UnifiedQA model and use it to perform QA on a multiple choice question without context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDyQ9_AjARwD",
        "outputId": "6ea863b2-5f47-4b50-e95e-ae7b16df1eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'Paris'}]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Using at least the base variant of the model is advised for good results\n",
        "generator = pipeline(\"text2text-generation\", model=\"allenai/unifiedqa-t5-base\")\n",
        "generator(\"What is the name of the city where the Eiffel Tower is located? \\n (A) Paris (B) London (C) Prague (D) Berlin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEHbEeXkARwD"
      },
      "source": [
        "### Your turn to Pipe\n",
        "\n",
        "Using the pipelines we saw in the tutorial, create a function taking a `question` string, an optional `context` string and an optional list of strings called `choices` and performs the following steps:\n",
        "\n",
        "- Use one of the [MarianMT](https://huggingface.co/models?search=helsinki-nlp) machine translation models to translate all the inputs from the language of your choice to English. You may want to split the text into sentences (e.g. by splitting on periods) to obtain better results for the context.\n",
        "\n",
        "- Use a paraphrasing model ([`tuner007/pegasus_paraphrase`](https://huggingface.co/tuner007/pegasus_paraphrase) is a good choice, albeit heavy) to produce 4 paraphrases of the question, using the `num_return_sequences` parameter.\n",
        "\n",
        "- For each question (translated + 4 paraphrases), format it with the translated context and choices (if present) as a single string in the format required by UnifiedQA.\n",
        "\n",
        "- Use the `allenai/unifiedqa-t5-base` model to generate an answer for each of the 5 questions.\n",
        "\n",
        "- If at least 3 of the 5 answers are identical strings, return the result translated back to the original language using the MarianMT model for the reciprocal language pair (e.g. if you used `Helsinki-NLP/opus-mt-nl-en` to translate from Dutch to English, you will need to use `Helsinki-NLP/opus-mt-en-nl`). Otherwise, print \"No common answer found\" translated in the original language.\n",
        "\n",
        "**Importantly**, the quality of the output does not determine your score in the evaluation. The goal is to get a feel for the models and their capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TLPYdZPbARwD",
        "outputId": "880a8f5b-5298-4a2f-be31-144247d3b8a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final answer: Sowjetunion\n"
          ]
        }
      ],
      "source": [
        "from typing import Optional, List\n",
        "from collections import Counter\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Set up pipelines\n",
        "translator_to_en = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-de-en\")\n",
        "translator_to_de = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-de\")\n",
        "paraphraser = pipeline(\"text2text-generation\", model=\"tuner007/pegasus_paraphrase\")\n",
        "qa_generator = pipeline(\"text2text-generation\", model=\"allenai/unifiedqa-t5-base\")\n",
        "\n",
        "def translate_text(text: str, translator) -> str:\n",
        "  sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
        "  if len(sentences) > 1:\n",
        "    translated_sentences = [translator(sentence)[0]['translation_text'] for sentence in sentences]\n",
        "    return '. '.join(translated_sentences)\n",
        "  else:\n",
        "    return translator(text)[0]['translation_text']\n",
        "\n",
        "\n",
        "def qa_input(question_en: str, context_en: Optional[str] = None, choices_en: Optional[List[str]] = None) -> str:\n",
        "  if context_en:\n",
        "    formatted = f\"{context_en}\\n{question_en}\"\n",
        "  else:\n",
        "    formatted = question_en\n",
        "\n",
        "  if choices_en:\n",
        "    labels = ['(A)', '(B)', '(C)', '(D)', '(E)', '(F)']\n",
        "    choices_str = \" \".join([f\"{labels[i]} {choice}\" for i, choice in enumerate(choices_en)])\n",
        "    formatted = formatted + \"\\n\" + choices_str\n",
        "\n",
        "  return formatted\n",
        "\n",
        "def answer(question: str, context: Optional[str] = None, choices: Optional[List[str]] = None):\n",
        "  # Translation\n",
        "    question_en = translator_to_en(question)[0]['translation_text']\n",
        "    context_en = translate_text(context, translator_to_en) if context else None\n",
        "    choices_en = [translator_to_en(choice)[0]['translation_text'] for choice in choices] if choices else None\n",
        "\n",
        "  # Paraphrasing\n",
        "    paraphrase_input = \"paraphrase: \" + question_en\n",
        "    paraphrased_outputs = paraphraser(\n",
        "        paraphrase_input,\n",
        "        max_length=60,\n",
        "        num_beams=10,\n",
        "        num_return_sequences=4,\n",
        "        do_sample=False\n",
        "    )\n",
        "    paraphrases = [output['generated_text'].strip() for output in paraphrased_outputs]\n",
        "    questions_list = [question_en] + paraphrases\n",
        "\n",
        "  # Formatting\n",
        "    formatted_inputs = [\n",
        "        qa_input(q, context_en=context_en, choices_en=choices_en)\n",
        "        for q in questions_list\n",
        "    ]\n",
        "\n",
        "  # Get answer\n",
        "    answers = []\n",
        "    for inp in formatted_inputs:\n",
        "      result = qa_generator(inp)\n",
        "      answer_text = result[0]['generated_text'].strip()\n",
        "      answers.append(answer_text)\n",
        "\n",
        "  # Consensus check\n",
        "    counts = Counter(answers)\n",
        "    common_answer, freq = counts.most_common(1)[0]\n",
        "    if freq >= 3:\n",
        "      final_answer = translator_to_de(common_answer)[0]['translation_text']\n",
        "      return final_answer\n",
        "    else:\n",
        "      no_common = translator_to_de(\"No common answer found\")[0]['translation_text']\n",
        "      return no_common\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  german_question = \"Welches Land brachte als erstes Menschen zum Mond?\"\n",
        "  german_context = \"Im Rahmen des Kalten Krieges standen die Raumfahrtprogramme der Weltmächte im Mittelpunkt eines erbitterten Wettlaufs ins All. Sowohl die Sowjetunion als auch die Vereinigten Staaten erzielten frühe Erfolge – von der Entsendung des ersten Satelliten bis hin zu bahnbrechenden bemannten Raumflügen. Doch eines dieser Länder gelang es, als erstes Menschen auf den Mond zu bringen, was als historischer Triumph und Symbol der technologischen Überlegenheit gefeiert wurde.\"\n",
        "  german_choices = [\"Vereinigte Staaten\", \"Sowjetunion\", \"China\", \"Frankreich\"]\n",
        "\n",
        "  final = answer(german_question, context=german_context, choices=german_choices)\n",
        "  print(\"Final answer:\", final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yV0T_n0ARwD"
      },
      "source": [
        "## (Optional) Exercise 3: SentenceTransformers for Semantic Similarity Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1RBWCF6ARwD"
      },
      "source": [
        "*Figures and some code are from the [CohereAI Semantic Search Tutorial](https://docs.cohere.ai/semantic-search/) by Jay Alammar.*\n",
        "\n",
        "> SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. The initial work is described in our paper [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084).\n",
        ">\n",
        "> You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.\n",
        ">\n",
        "> The framework is based on PyTorch and Transformers and offers a large collection of pre-trained models tuned for various tasks. Further, it is easy to fine-tune your own models.\n",
        "\n",
        "Semantic search is a typical use case in natural language processing, in which we want to retrieve the most relevant documents from a corpus (e.g. Automatic FAQs, Web browser search results, etc.). Using the similarity between embedded text representations allows us to go beyond simple keyword matching, which is highly desirable in this setting (e.g. `tomorrow will rain` should be very close in embedding space to `the weather forecast announces showers for the next day`, despite no lexical overlap).\n",
        "\n",
        "<div>\n",
        "<img alt=\"Visualizing Semantic Search\" src=\"https://github.com/cohere-ai/notebooks/raw/main/notebooks/images/basic-semantic-search-overview.png?3\" style=\"width: 60%\" />\n",
        "</div>\n",
        "\n",
        "In this exercise we will use first Huggingface Transformers and then the [SentenceTransformers](https://sbert.net) library to find the most relevant paragraphs for a specific query.\n",
        "\n",
        "Let's start by loading the `train` split of the `squad` dataset from the Dataset Hub and flatten its structure so that every example contains a single triplet `(context, question, answer)`. We are going to use only the first 50 rows of the dataset, the others can be discarded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZ2TBSfmARwE"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "squad = load_dataset(\"squad\", split=\"train[:50]\")\n",
        "\n",
        "squad_train_filtered = None# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIpzgOOrARwE"
      },
      "source": [
        "Now, sample a question at random from the resulting dataset and print it. You can use `shuffle` or turn the Dataset into a `DataFrame` and use `sample`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41FpXJ3TARwE"
      },
      "outputs": [],
      "source": [
        "query = None # Your code here\n",
        "\n",
        "print(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svltnzM5ARwE"
      },
      "source": [
        "We now going to use a model trained to perform **semantic search** to retrieve the top 10 most likely contexts for each selected question.\n",
        "\n",
        "The model `sentence-transformers/multi-qa-MiniLM-L6-cos-v1` is a good choice for this task, since it is relatively small and was explicitly trained for semantic search. We are going to define now three utility functions:\n",
        "\n",
        "- `dot_score` computes the dot product between two Pytorch tensors.\n",
        "- `mean_pooling` averages the embeddings produced by a model to obtain a **sentence embedding**.\n",
        "- `encode` uses a model and a tokenizer to convert a list of texts into a tensor of embeddings. **Complete it with the first two steps we saw in the tutorial.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLNdCB9NARwE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def dot_score(a: Tensor, b: Tensor):\n",
        "    \"\"\"\n",
        "    Computes the dot-product dot_prod(a[i], b[j]) for all i and j.\n",
        "    :return: Matrix with res[i][j]  = dot_prod(a[i], b[j])\n",
        "    Taken from the SentenceTransformer library\n",
        "    \"\"\"\n",
        "    if not isinstance(a, torch.Tensor):\n",
        "        a = torch.tensor(a)\n",
        "    if not isinstance(b, torch.Tensor):\n",
        "        b = torch.tensor(b)\n",
        "    if len(a.shape) == 1:\n",
        "        a = a.unsqueeze(0)\n",
        "    if len(b.shape) == 1:\n",
        "        b = b.unsqueeze(0)\n",
        "    print(a.shape, b.shape)\n",
        "    # Compute the dot-product\n",
        "    return torch.mm(a, b.transpose(0, 1))\n",
        "\n",
        "\n",
        "#Mean Pooling - Average all the embeddings produced by the model\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    # First element of model_output contains all token embeddings\n",
        "    token_embeddings = model_output.last_hidden_state\n",
        "    # Expand the mask to the same size as the token embeddings to avoid indexing errors\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    # Compute the mean of the token embeddings\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "\n",
        "#Encode text\n",
        "def encode(model, tokenizer, texts):\n",
        "    # Tokenize sentences\n",
        "    encoded_input = None # Your code here\n",
        "    # Compute token embeddings\n",
        "    with torch.no_grad():\n",
        "        model_output = None # Your code here\n",
        "    # Perform pooling\n",
        "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "    # Normalize embeddings\n",
        "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyWiTMngARwE"
      },
      "source": [
        "Let's now use these functions to compute similarity scores for the query and all the contexts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_omegNdARwE"
      },
      "outputs": [],
      "source": [
        "# Sentences we want sentence embeddings for\n",
        "contexts = list(squad_train_filtered.to_pandas()['context'])\n",
        "\n",
        "# Load the model and tokenizer from HuggingFace Hub\n",
        "tokenizer = None # Your code here\n",
        "model = None # Your code here\n",
        "\n",
        "#Encode query and contexts with the encode function\n",
        "query_emb = None # Your code here\n",
        "contexts_emb = None # Your code here\n",
        "\n",
        "#Compute dot score between query and all contexts embeddings\n",
        "scores = torch.mm(query_emb, contexts_emb.transpose(0, 1))[0].cpu().tolist()\n",
        "\n",
        "#Combine contexts & scores\n",
        "contexts_score_pairs = list(zip(contexts, scores))\n",
        "\n",
        "#Sort by decreasing score\n",
        "contexts_score_pairs = sorted(contexts_score_pairs, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "#Output passages & scores\n",
        "for ctx, score in contexts_score_pairs:\n",
        "    print(score, ctx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UGpsP_WARwE"
      },
      "source": [
        "We can now use SentenceTransformers to do the same, but using much less code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOmYfon5ARwE"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Query was defined above\n",
        "contexts = list(squad_train_filtered.to_pandas()['context'])\n",
        "\n",
        "#Load the model\n",
        "model = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')\n",
        "\n",
        "#Encode query and contexts using SentenceTransformer model.encode\n",
        "query_emb = model.encode(query)\n",
        "contexts_emb = model.encode(contexts)\n",
        "\n",
        "#Compute dot score between query and all contexts embeddings\n",
        "scores = util.dot_score(query_emb, contexts_emb)[0].tolist()\n",
        "\n",
        "#Combine contexts & scores\n",
        "contexts_score_pairs = list(zip(contexts, scores))\n",
        "\n",
        "#Sort by decreasing score\n",
        "contexts_score_pairs = sorted(contexts_score_pairs, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "#Output passages & scores\n",
        "for ctx, score in contexts_score_pairs:\n",
        "    print(score, ctx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWQL1jd2ARwE"
      },
      "source": [
        "For a more advanced overview on how to optimize speed with semantic search, see how to use the [FAISS](https://github.com/facebookresearch/faiss) library to perform fast nearest neighbor search natively with Huggingface Transformers here: [Using FAISS for efficient similarity search ](https://huggingface.co/course/chapter5/6?fw=pt#using-faiss-for-efficient-similarity-search)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "ead1b95f633dc9c51826328e1846203f51a198c6fb5f2884a80417ba131d4e82"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}